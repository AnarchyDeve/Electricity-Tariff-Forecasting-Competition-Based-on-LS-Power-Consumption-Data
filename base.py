# ================================================================
# âš¡ Electricity Forecast Full Pipeline (ì‹œê³„ì—´ + Optuna + TensorBoard)
# ================================================================
import os, itertools, datetime, json, math, random, threading, webbrowser
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import (
    Input, LSTM, GRU, Dense, Dropout, BatchNormalization, Bidirectional
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard
from tensorflow.keras.optimizers.schedules import CosineDecayRestarts
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error
import optuna
from tensorboard import program

# ===================================================
# ğŸ”¥ TensorBoard ìë™ ì‹¤í–‰
# ===================================================
def launch_tensorboard(logdir="logs", port=6006):
    tb = program.TensorBoard()
    tb.configure(argv=[None, "--logdir", logdir, "--port", str(port)])
    url = tb.launch()
    print(f"\nğŸš€ TensorBoard ì‹¤í–‰ ì¤‘: {url}")
    webbrowser.open(url)
    return tb

tb_thread = threading.Thread(target=launch_tensorboard, kwargs={"logdir": "logs", "port": 6006})
tb_thread.daemon = True
tb_thread.start()

# ===================================================
# 0ï¸âƒ£ ê²½ë¡œ ì„¤ì •
# ===================================================
train_path = "./data/fixed_train_clean.csv"
test_path  = "./data/fixed_test_weather_full.csv"

TARGETS_STEP1 = [
    "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
    "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§€ìƒì—­ë¥ (%)",
    "ì§„ìƒì—­ë¥ (%)",
]
TARGET_STEP2 = "ì „ê¸°ìš”ê¸ˆ(ì›)"

TIME_FEATURES = ["sin_day","cos_day","day_of_week","is_weekend","is_holiday"]
WEATHER_FEATURES = ["ê¸°ì˜¨(Â°C)","ìŠµë„(%)","í’ì†(m/s)","ì´ê°•ìˆ˜ëŸ‰(mm)"]
ALL_FEATURES = TIME_FEATURES + WEATHER_FEATURES

N_SPLITS = 3
N_TRIALS_STEP1 = 15
N_TRIALS_STEP2 = 15
ENSEMBLE_SEEDS = [7, 13, 29]

os.makedirs("logs", exist_ok=True)
os.makedirs("artifacts", exist_ok=True)

# ===================================================
# 1ï¸âƒ£ ìœ í‹¸ í•¨ìˆ˜
# ===================================================
def set_global_seed(seed: int):
    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)

def create_sequences(X, y, seq_len):
    Xs, ys = [], []
    for i in range(len(X) - seq_len):
        Xs.append(X[i:i+seq_len])
        ys.append(y[i+seq_len])
    return np.asarray(Xs), np.asarray(ys)

def attention_block(x):
    h = Dense(x.shape[-1], activation="tanh")(x)
    e = Dense(1)(h)
    a = tf.nn.softmax(e, axis=1)
    context = tf.reduce_sum(a * x, axis=1)
    return context
def build_model(input_shape, params, model_type="LSTM", use_attention=True, output_dim=1):
    inp = Input(shape=input_shape)
    RNN = LSTM if model_type in ["LSTM", "BiLSTM"] else GRU

    # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
    x = RNN(params["units1"], return_sequences=True,
            dropout=params["dropout"],
            recurrent_dropout=params.get("recurrent_dropout", 0.0))(inp)
    if params.get("use_batchnorm", False):
        x = BatchNormalization()(x)

    # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ
    x = RNN(params["units2"], return_sequences=True,
            dropout=params["dropout"],
            recurrent_dropout=params.get("recurrent_dropout", 0.0))(x)
    if params.get("use_batchnorm", False):
        x = BatchNormalization()(x)

    # ğŸ”¹ ì„¸ ë²ˆì§¸ ì€ë‹‰ì¸µ ì¶”ê°€
    x = RNN(params.get("units3", 64), return_sequences=True,
            dropout=params["dropout"],
            recurrent_dropout=params.get("recurrent_dropout", 0.0))(x)
    if params.get("use_batchnorm", False):
        x = BatchNormalization()(x)

    # Attention or Global Pooling
    if use_attention:
        x = attention_block(x)
    else:
        x = tf.keras.layers.GlobalAveragePooling1D()(x)

    x = Dropout(params["dropout"])(x)
    out = Dense(output_dim)(x)

    opt = tf.keras.optimizers.Adam(learning_rate=params["lr"])
    model = Model(inp, out)
    model.compile(optimizer=opt, loss="mae")
    return model

def tensorboard_dir(step_name, trial_no=None, fold=None, tag="fit"):
    parts = [step_name]
    if trial_no is not None: parts.append(f"trial_{trial_no}")
    if fold is not None: parts.append(f"fold_{fold}")
    parts.append(datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    return os.path.join("logs", "_".join(parts), tag)

# ===================================================
# 2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬
# ===================================================
def preprocess_dataframe(df: pd.DataFrame):
    df = df.copy()

    # ì‹œê³„ì—´ ì •ë ¬
    datetime_col = None
    for col in ["ì¸¡ì •ì¼ì‹œ_x", "ì¸¡ì •ì¼ì‹œ_y"]:
        if col in df.columns:
            datetime_col = col
            break
    if datetime_col:
        df[datetime_col] = pd.to_datetime(df[datetime_col], errors="coerce")
        df = df.sort_values(by=datetime_col).reset_index(drop=True)

    # ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
    drop_cols = ["id", "ì¸¡ì •ì¼ì‹œ_x", "ì¸¡ì •ì¼ì‹œ_y"]
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

    # season â†’ ì›í•«
    if "season" in df.columns:
        season_map = {"spring": 0, "summer": 1, "autumn": 2, "fall": 2, "winter": 3}
        df["season"] = df["season"].map(season_map).fillna(3).astype(int)
        df = pd.get_dummies(df, columns=["season"], prefix="season")

    # ì‘ì—…ìœ í˜• â†’ ì›í•«
    if "ì‘ì—…ìœ í˜•" in df.columns:
        df = pd.get_dummies(df, columns=["ì‘ì—…ìœ í˜•"], prefix="ì‘ì—…ìœ í˜•")

    # ë‚ ì”¨ì½”ë“œ â†’ Label Encoding
    if "ë‚ ì”¨ì½”ë“œ" in df.columns:
        le = LabelEncoder()
        df["ë‚ ì”¨ì½”ë“œ"] = le.fit_transform(df["ë‚ ì”¨ì½”ë“œ"].astype(str))

    # ë‚˜ë¨¸ì§€ object ì»¬ëŸ¼ ìë™ ì¸ì½”ë”©
    for col in df.columns:
        if df[col].dtype == "object":
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))

    return df

train = pd.read_csv(train_path)
test = pd.read_csv(test_path)
train = preprocess_dataframe(train)
test = preprocess_dataframe(test)

# ìŠ¤ì¼€ì¼ë§
X_all = train.select_dtypes(include=[np.number]).drop(columns=TARGETS_STEP1 + [TARGET_STEP2], errors="ignore")
scaler_X = StandardScaler()
X_scaled_full = scaler_X.fit_transform(X_all.values)

# ===================================================
# 3ï¸âƒ£ Optuna ëª©ì  í•¨ìˆ˜
# ===================================================
def objective(trial, X, y, step_name, feature_names):
    k = trial.suggest_int("n_features", max(3, len(feature_names)//2), len(feature_names))
    chosen = trial.suggest_categorical("features", random.sample(list(itertools.combinations(feature_names, k)), min(120, math.comb(len(feature_names), k))))
    feat_idx = [feature_names.index(f) for f in chosen]
    X_sel = X[:, feat_idx]

    params = {
        "model_type": trial.suggest_categorical("model_type", ["LSTM","GRU","BiLSTM","BiGRU"]),
        "units1": trial.suggest_int("units1", 32, 128, step=16),
        "units2": trial.suggest_int("units2", 16, 96, step=16),
        "dropout": trial.suggest_float("dropout", 0.1, 0.5, step=0.1),
        "recurrent_dropout": trial.suggest_float("recurrent_dropout", 0.0, 0.3, step=0.1),
        "use_batchnorm": trial.suggest_categorical("use_batchnorm", [True, False]),
        "seq_len": trial.suggest_int("seq_len", 12, 96, step=12),
        "optimizer": trial.suggest_categorical("optimizer", ["adam","nadam","adamw"]),
        "lr": trial.suggest_float("lr", 1e-4, 5e-3, log=True),
        "batch_size": trial.suggest_categorical("batch_size", [16,32,64]),
        "epochs": trial.suggest_int("epochs", 30, 100, step=10),
        "lr_scheduler": trial.suggest_categorical("lr_scheduler", ["plateau","cosine"]),
        "cosine_steps": trial.suggest_int("cosine_steps", 300, 2000, step=100),
        "use_attention": trial.suggest_categorical("use_attention", [True, False]),
    }

    tscv = TimeSeriesSplit(n_splits=N_SPLITS)
    fold_mae = []

    for fold, (tr_idx, va_idx) in enumerate(tscv.split(X_sel)):
        X_tr, y_tr = X_sel[tr_idx], y[tr_idx]
        X_va, y_va = X_sel[va_idx], y[va_idx]
        X_tr_seq, y_tr_seq = create_sequences(X_tr, y_tr, params["seq_len"])
        X_va_seq, y_va_seq = create_sequences(X_va, y_va, params["seq_len"])

        model = build_model((params["seq_len"], X_sel.shape[1]), params, params["model_type"], params["use_attention"])
        cbs = [TensorBoard(log_dir=tensorboard_dir(step_name, trial.number, fold)), EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)]
        if params["lr_scheduler"] == "plateau":
            cbs.append(ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, min_lr=1e-5))

        model.fit(X_tr_seq, y_tr_seq, validation_data=(X_va_seq, y_va_seq),
                  epochs=params["epochs"], batch_size=params["batch_size"], verbose=0, callbacks=cbs)

        y_hat = model.predict(X_va_seq, verbose=0)
        fold_mae.append(mean_absolute_error(y_va_seq, y_hat))

    trial.set_user_attr("params", params)
    trial.set_user_attr("features", chosen)
    return float(np.mean(fold_mae))
